\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{chemfig}
\usepackage[version=4]{mhchem}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{mathabx}
\usepackage{relsize}
\usepackage{graphics}
\usepackage{outlines}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
%\usepackage{indentfirst}
\usepackage{tikz}
\usepackage{listings}
\usepackage{sidecap}
\usepackage{comment}
\geometry{
 a4paper,
 total={6.5in,0in},
 left= 15mm,
 top= 15mm,
 bottom=15mm,
 right = 15mm
 }
\title{Energy Log}
\author{Marcos Perez}
\date{June 2022 - }

\begin{document}

\maketitle

\section{June-ish}
\subsubsection{5/31/22}
Configuring \href{https://docs.github.com/en/repositories/working-with-files/managing-large-files/configuring-git-large-file-storage}{https://docs.github.com/en/repositories/working-with-files/managing-large-files/configuring-git-large-file-storage}\\
Currently pushing some datasets to the Github repository. \\
It worked! :)\\
Note from the future (7/25) I went way past the 5 GB maximum and decided to delete it. Will instead use a Google Drive account to host all the data as a back up.
\subsubsection{6/13/2022}
Downloaded ENDF-Libraries. Extracting all of the files from subfolders automatically using 7-zip :)\\
Added the page where I downloaded everything to list of links. Trying the EMPIRE software for simulations to see if its helpful. I suspect it will be. \\
Ended up deleting it because I don't think it will be useful. \\
Downloaded \href{https://www.nist.gov/pml/atomic-weights-and-isotopic-compositions-relative-atomic-masses}{isotopic abundances} from the National Institute of Standards and Technology. 
\subsection{6/27/2022}
I should do math using Sage Notebooks Using the gruvbox theme in sage :) \\
\subsubsection{6/30/22}
Just had a meeting with Bethany. I could include the shielding in the mass by keeping track of the radiation type and their contribution to the power density. \\
Now that I've been using a variety of tools for data analysis and computational physics, I've decided the following are the advantages of each:
\begin{itemize}
    \item Speed, ease of use - Julia
    \item Package ecosystem, ease of use - Python (pypy can yield some speed improvements but will always lag behind Julia)
    \item Colab - make jupyter notebooks more accessible and easily shared
    \item Git + Github - use it. The desktop version is more user friendly and thorough but less reliable. For more straightforward applications, the command line version is faster and more than worth setting up for long term projects. 
    \item Streamlit - for interactive data visualizations you want to exist in the world, this is much easier to use than Heroku. 
    \item Heroku - longer lived than streamlit, and I suspect will outlive it. 
    \item Dash + Plotly - awesome in Python, needs some work in Julia. 
\end{itemize}

Since I already have the beta decay fraction, maybe I could just filter out the isotopes that can directly emit non-beta radiation? It's so hard to go between all these files. I should write down the whole process as a flow chart and put it into a singular notebook. \\
Did that for decay chains that only decay via beta decay. Have to refine to exclude gammas and unstable daughter products. Need to repeat for all decay types. Shielding!\\
\section{July}
\subsubsection{7/5/2022}
Making a functional backup on colab \href{https://colab.research.google.com/drive/1rXPnMapuznZOmF3p908jdk1eiosalbjV?usp=sharing}{https://colab.research.google.com/drive/1rXPnMapuznZOmF3p908jdk1eiosalbjV?usp=sharing}\\

What is the decay energy I've been using the power densities? It should be the sum of the beta decay and any gamma emissions. Do the project multiple ways until and see how much the results agree.\\
Finally found a way to describe my project: simulating radioisotope production for energy storage. Data driven optimization of nuclear medicine. 
\textbf{Different Analyses and Data Sources}
\begin{enumerate}
    \item \href{https://www-nds.iaea.org/amdc/}{Nubase 2020 + AME 2020} + \href{https://nds.iaea.org/relnsd/vcharthtml/api_v0_guide.html}{Livechart} $\to$ decay chains with any of the following decay modes: \\
    a: alpha decay beta plus decay and electron capture, bm: beta minus decay, g: gamma emission, e: Auger and conversion electron, x: X-ray emission
    \item \href{https://www-nds.iaea.org/amdc/}{Nubase 2020 + AME 2020} + \href{https://www.doseinfo-radar.com/RADARDecay.html}{RADAR} $\to$ decay chains that only include the decay modes $\beta^-$ decay and gamma emission (except for the final daughter nucleus, which can decay by any mode or even be stable).
\end{enumerate}
The only thing standing between seamlessly using the same code on Colab and my laptop is the file system. I need to write a function to handle that. Left off in the livecharts notebook on Github and the laptop and  \href{https://colab.research.google.com/drive/1rXPnMapuznZOmF3p908jdk1eiosalbjV?usp=sharing}{this colab notebook}\\
\includegraphics[scale=.4]{Images/concept setup.PNG}\\
In this setup, a gamma ray will be fired at a neutron donor (blue cylinder) which would then be captured by the neutron acceptor (red surroundings). Ideally, the neutron donor would have a very small radius but be a very long cylinder. Assuming every gamma ray produces a neutron which is then capture by the neutron acceptor, it would only cost 1 MeV/neutron with deuterium as a neutron donor (leading an energy storage efficiency of $~\frac{1}{3}$ based on the power density of decay chains work since one traverse along each chain yields $300$ keV). Can it's low cross section and density be overcome with this design? What is the ideal neutron acceptor for each neutron donor? Is this less efficient than firing protons into the neutron acceptor? 
\subsubsection{7/25/2022}
Found the readmes for the complete data libraries and found the most recent libraries \\
\subsubsection{7/26}
Doing more data wrangling using the link with the superset of all the data in the list of links file. Will combine all of the data into a singular directory for each projectile. 
\section{August}
\subsubsection{8/9/2022}
Installed and setup talys in a subdirectory of my downloads folder using WSL. In the process, I downloaded gfortran. Now have a talys executable in the bin directory of WSL. \\
Also automated and the use of the DPASS GUI to retrieve data. Takes $<$ 6 hours to retrieve the entire database. \\

\subsubsection{8/11/2022}
Made a lot more progress on the code. Wondering if radiative capture and neutron absorption are the only two competing reactions? They are certianly a subset of them. (screenshot from endf6 manual) \\
\includegraphics[scale=.4]{Images/are these the only two competing reactions.PNG}\\
I don't care about the specific reaction so much as the residual products including the desired product isotope. 

\subsubsection{8/15/2022}
Been reading \href{https://www-nds.iaea.org/exfor}{exfor} a lot recently. \\
very pleasantly surprised by some of the gamma ray cross sections of Fe-58. On the order of .001 barns at .01 MeV for the emission a neutron. We have 
\begin{equation}
    k = \frac{\rho\sigma N_A}{M}
\end{equation}
where $M$ is the molar mass in kg, $N_A$ is Avogadro's constant, $\sigma$ is the cross section in m$^2$ and $\rho$ is the mass density in kg/m$^3$. Note that for iron, $\rho N_A/M\approx 6\times10^{26}$.
Based on "On the 58Fe($\gamma$,n)57Fe reaction near a threshold" by Kitaev et al, there is a surprisingly low energy peak of 0.038 barns at 0.00597 MeV. 
$k\approx  6\sigma\times10^{26} approx  10^{-3}$. Thus, for a \textbf{thin} target it would take approximately 6 MeV to produce each neutron. Either the data is wrong or I am reading it wrong (maybe it is actually meant to be in GeV??). According to both ENDF and TENDL, it cross sections should peak at .096 barns at 19 MeV, which makes much more since when considering the nuclear binding energy that must be overcome in the $(\gamma,n)$ reaction. 
\subsubsection{August 26th}
After months of consideration I've decided on the following for my workflow: 
\begin{outline}
\1 Repetitive or computationally expensive tasks
    \2 Code these parts using the Julia language rather than Python. While I'm writing it, I can even do so in a Jupyter notebook, I just need to set the kernel to the Julia executable. There is a shortcut to do exactly this in VS code, so it is very convenient to go between kernels. 
    \2 By uploading such a notebook to Google Colab, then downloading it as a .py file, then resaving it as a .jl file, it is very fast to convert Julia Jupyter notebook into a julia script. 
    \2 \href{https://colab.research.google.com/drive/1vUglHFSJJcE75oV5qs9fQFAh6yQPVIiF?usp=sharing}{
    Example of how to run individual cells of Julia in Colab}. One can easily just call these julia scripts while invoking the Julia executable from a Python Jupyter notebook. Note that some packages are not compatible with voila and Heroku web apps, but they will work on colab as seen in the linked example. 
\end{outline}


\end{document}
